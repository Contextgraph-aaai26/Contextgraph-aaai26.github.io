<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ContextGraph: Lifelog Intelligence Framework</title>
    <style>
        /* --- Basic Setup --- */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            line-height: 1.6;
            background-color: #fdfdfd;
            color: #333;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1000px;
            margin: 20px auto;
            padding: 0 20px;
        }

        /* --- Header --- */
        header {
            text-align: center;
            border-bottom: 2px solid #eee;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        h1 {
            font-size: 2.8em;
            font-weight: 600;
            margin-bottom: 10px;
            color: #111;
        }
        .authors {
            font-size: 1.15em;
            margin-bottom: 10px;
        }
        .affiliation {
            font-size: 0.95em;
            color: #555;
            margin-top: 5px;
        }
        .conference {
            font-size: 1.2em;
            font-weight: 500;
            color: #0056b3; /* A strong blue */
            margin-top: 15px;
        }

        /* --- Links --- */
        .links {
            text-align: center;
            margin: 30px 0;
        }
        .links a {
            display: inline-block;
            text-decoration: none;
            font-weight: 600;
            font-size: 1.1em;
            background-color: #007bff;
            color: #ffffff;
            padding: 12px 20px;
            margin: 5px 10px;
            border-radius: 8px;
            transition: background-color 0.3s ease;
        }
        .links a:hover {
            background-color: #0056b3;
        }
        .links a.code {
            background-color: #28a745; /* Green for Code */
        }
        .links a.code:hover {
            background-color: #218838;
        }

        /* --- Sections --- */
        section {
            margin-bottom: 50px;
        }
        h2 {
            font-size: 2em;
            font-weight: 500;
            border-bottom: 1px solid #ddd;
            padding-bottom: 8px;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        h3 {
            font-size: 1.4em;
            font-weight: 600;
            margin-top: 30px;
            color: #444;
        }
        p, li {
            font-size: 1.05em;
            color: #444;
        }
        
        /* --- Images --- */
        figure {
            margin: 30px 0;
            text-align: center;
        }
        img.teaser {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        img.architecture {
            max-width: 100%;
            height: auto;
            border: 1px solid #eee;
            border-radius: 8px;
            margin-bottom: 10px;
        }
        figcaption {
            font-size: 0.9em;
            color: #666;
            margin-top: 10px;
            font-style: italic;
        }
        
        /* --- Results --- */
        .results-list {
            list-style-type: none;
            padding-left: 0;
        }
        .results-list li {
            position: relative;
            padding-left: 35px;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        .results-list li:before {
            content: '✅';
            position: absolute;
            left: 0;
            top: 0;
            font-size: 1.2em;
        }
        .results-list li strong {
            color: #0056b3;
        }

        /* --- Citation --- */
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow-x: auto;
            font-family: "Courier New", Courier, monospace;
            font-size: 0.95em;
            line-height: 1.5;
        }
    </style>
</head>
<body>

    <div class="container">

        <header>
            <h1>ContextGraph: Lifelog Intelligence Framework for Contextual Subgraph Evolution</h1>
            
            <div class="authors">
                Anil Sharma, Gunturi Venkata Sai Phani Kiran, Jayesh Rajkumar Vachhani, Sourabh Vasant Gothe, <br>
                Ayon Chattopadhyay, Yashwant Saini, Parameswaranath Vadackupurath Mani, Barath Raj Kandur Raja
            </div>
            
            <div class="affiliation">
                Samsung R&D Institute India-Bangalore (SRI-B)
            </div>

            <div class="conference">
                Accepted at AAAI 2026 (Main Technical Track)
            </div>
        </header>

        <section class="teaser">
            <figure>
                <img src="images/teaser.png" alt="ContextGraph Framework Overview: From Raw Logs to Insights" class="teaser">
                <figcaption>
                    Figure 1: ContextGraph transforms raw lifelogs into a Temporal Knowledge Graph (TKG) to enable applications like Routine Evolution tracking and New Interest Detection.
                </figcaption>
            </figure>
        </section>

        <section class="links">
            <a href="ContextGraph_AAAI2026.pdf" target="_blank">Paper (PDF)</a>
            <a href="ContextGraph_Supp.pdf" target="_blank">Supplementary</a>
            <a href="poster.pdf" target="_blank">Poster</a>
            <a href="https://github.com/your-username/contextgraph" target="_blank" class="code">Code</a>
        </section>

        <section id="abstract">
            <h2>Abstract</h2>
            <p>
                Lifelogging involves the continuous and comprehensive recording of a user's daily activities, behaviors, and interactions, offering valuable insights for personalized healthcare, event retrieval, and lifestyle analysis. However, extracting meaningful patterns from lifelog data requires models to capture deeper temporal contexts beyond simple retrieval. To address this, we introduce <strong>ContextGraph</strong>, a lifelog intelligence framework that models lifelogs as a Temporal Knowledge Graph (TKG) to reason about the user's evolving life patterns over time. ContextGraph computes Day Context Embeddings (DCE) to encode the temporal spread and social scene context of a user's daily behavior. Then, a novel Lens module extracts semantically meaningful subgraph snapshots around an anchor node in the TKG, representing specific personal contexts. The Lens module computes an evolution signature for each subgraph, indicating whether it is growing, decaying, or remaining static. By analyzing these evolution signatures, ContextGraph provides actionable insights into the user's lifelogs such as stable routines, behavioral drifts, or lifestyle changes.
            </p>
        </section>

        <section id="method">
            <h2>Our Framework</h2>
            <p>
                ContextGraph transforms chaotic, multimodal data (locations, apps, calls) into actionable intelligence through a structured three-stage pipeline.
            </p>

            <h3>1. From Raw Data to Temporal Knowledge Graph (TKG)</h3>
            <p>
                The foundation of our framework is the automated mapping of raw lifelogs into a structured TKG. As illustrated in the left panel of <strong>Figure 1</strong> (above), we process heterogeneous data streams—including GPS coordinates, app usage logs, call records, and WiFi connections.
            </p>
            <p>
                Each raw event is mapped to an entity node (e.g., "Gym", "WhatsApp", "Mom") and connected via temporal edges that preserve the sequence and duration of interactions. This transformation converts a disjointed stream of logs into a rich, interconnected graph that explicitly represents the relationships between a user's activities, locations, and social interactions over time.
            </p>

            <h3>2. Day Context Embeddings (DCE)</h3>
            <p>
                To understand the "vibe" of each day, we developed a novel deep learning model to create a unique fingerprint (embedding) for every 24-hour period. As shown below, the DCE architecture encodes both the <strong>temporal rhythm</strong> (distribution of activities) and the <strong>social scene context</strong> (interactions and environment).
            </p>
            <figure>
                <img src="images/dce_architecture.png" alt="Day Context Embedding (DCE) Architecture" class="architecture">
                <figcaption>
                    Figure 2: The DCE Architecture. It utilizes a Cyclic Encoder for temporal features and a Heterogeneous Graph Attention Network for scene context, fused via a Context-GVAE.
                </figcaption>
            </figure>

            <h3>3. The Lens Module</h3>
            <p>
                This is our reasoning engine. While the TKG structures the data, the Lens Module allows us to "zoom in" on specific contexts (e.g., "Fitness" or "Work"). It identifies an <strong>Anchor Node</strong>, extracts the relevant <strong>Contextual Subgraph</strong>, and computes an <strong>Evolution Signature</strong> to classify the routine as <em>Growing</em>, <em>Decaying</em>, or <em>Static</em>.
            </p>
            <figure>
                <img src="images/lens_module.png" alt="Lens Module Architecture and Mechanism" class="architecture">
                <figcaption>
                    Figure 3: The Lens Module mechanism for Anchor Selection, Subgraph Extraction, and Evolution Signature computation.
                </figcaption>
            </figure>
        </section>

        <section id="results">
            <h2>Key Results</h2>
            <p>We rigorously tested ContextGraph on real-world and simulated data. Key highlights include:</p>
            
            <ul class="results-list">
                <li>
                    <strong>Identifies Real-World Behavioral Shifts:</strong> In a real-world travel case study, ContextGraph successfully detected the decay of a user's "home" routine and the growth of a new "travel" routine, achieving a <strong>28% higher F1-score</strong> than the closest baseline method.
                </li>
                <li>
                    <strong>Robust to Real-World Noise:</strong> Lifelog data is messy and sparse. We simulated this by removing up to 30% of the data. Our DCE module maintained high performance, achieving a <strong>0.88 ROC AUC</strong>, proving it's robust enough for real-world use.
                </li>
                <li>
                    <strong>Accurate Subgraph Identification:</strong> The Lens module's ability to find the right behaviors is critical. Our anchor detection method outperformed four other baselines, achieving a <strong>Jaccard Score of 0.82</strong>.
                </li>
            </ul>
        </section>

        <section id="citation">
            <h2>Citation (BibTeX)</h2>
            <p>If you find our work useful in your research, please consider citing:</p>
            <pre>
@inproceedings{sharma2026contextgraph,
    title     = {ContextGraph: Lifelog Intelligence Framework for Contextual Subgraph Evolution},
    author    = {Sharma, Anil and Kiran, Gunturi Venkata Sai Phani and Vachhani, Jayesh Rajkumar and Gothe, Sourabh Vasant and Chattopadhyay, Ayon and Saini, Yashwant and Mani, Parameswaranath Vadackupurath and Raja, Barath Raj Kandur},
    booktitle = {Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI)},
    year      = {2026},
    note      = {Main Technical Track}
}
            </pre>
        </section>

    </div> </body>
</html>
